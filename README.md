### **1. LLM Basics**

**1.1 Tokenization**
* **1.1.1 Word-based tokenization**
* **1.1.2 Character-based tokenization**
* **1.1.3 Subword-based tokenization**
    * **1.1.3.1 BPE (Byte-Pair Encoding)**
    * **1.1.3.2 BBPE (Byte-level BPE)**
    * **1.1.3.3 WordPiece**
    * **1.1.3.4 Unigram**
    * **1.1.3.5 SentencePiece**

**1.2 Embedding**
* **1.2.1 History**
    * **1.2.1.1 One-hot Encoding**
    * **1.2.1.2 Co-occurrence Matrix**
    * **1.2.1.3 Distributed Word Representation**
* **1.2.2 Static Embeddings**
    * **1.2.2.1 Word2Vec**
    * **1.2.2.2 GloVe**
    * **1.2.2.3 FastText**
* **1.2.3 Contextual Embeddings**

**1.3 Positional Encoding**
* **1.3.1 Learnable Positional Embedding**
* **1.3.2 Sinusoidal Positional Encoding**
* **1.3.3 Bucketed Relative Position Bias**
* **1.3.4 ALiBi**
* **1.3.5 RoPE**
* **1.3.6 Length Extrapolation**
    * **1.3.6.1 NTK (Neural Tangent Kernel)**
    * **1.3.6.2 YaRN**
    * **1.3.6.3 Dual-Chunk Attention**
    * **1.3.6.3 Other Methods**
 
**1.4 Attention**
